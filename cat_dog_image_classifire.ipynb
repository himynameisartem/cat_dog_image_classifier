{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169684a2-6454-417e-961a-2b0f13a10391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f07f53-b042-4fa3-aca4-bd4d9d3f44e5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c7ef8-d8cd-4d23-b315-866d9ce65b22",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af76fbc-18d7-4e75-a415-052b2e1f6fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cat-and-dog.zip', <http.client.HTTPMessage at 0x30983ddd0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url = \"https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\"\n",
    "zip_path = \"cat-and-dog.zip\"\n",
    "\n",
    "urllib.request.urlretrieve(url, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ae7645-e668-4772-9150-57bfbd229714",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_path = \"./temp\"\n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489e4eb-9973-4eaf-b92b-7913f014eb60",
   "metadata": {},
   "source": [
    "## Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07245df-6b4f-4e42-9f7b-a30f17cb371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './temp/training_set/training_set/'\n",
    "BASE_DIR = './dataset/'\n",
    "\n",
    "CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
    "CLASS_COUNT = len(CLASS_LIST)\n",
    "\n",
    "if os.path.exists(BASE_DIR):\n",
    "    shutil.rmtree(BASE_DIR)\n",
    "\n",
    "os.mkdir(BASE_DIR)\n",
    "\n",
    "train_dir = os.path.join(BASE_DIR, 'train')\n",
    "validation_dir = os.path.join(BASE_DIR, 'validation')\n",
    "test_dir = os.path.join(BASE_DIR, 'test')\n",
    "\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(validation_dir)\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0039bf65-bdb0-42f9-8171-c5d41b5ea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    img_path: str,\n",
    "    new_path: str,\n",
    "    class_name: str,\n",
    "    start_index: int,\n",
    "    end_index: int\n",
    "):\n",
    "    src_path = os.path.join(img_path, class_name)\n",
    "    dst_path = os.path.join(new_path, class_name)\n",
    "\n",
    "    class_files = os.listdir(src_path)\n",
    "\n",
    "    os.mkdir(dst_path)\n",
    "\n",
    "    for fname in class_files[start_index:end_index]:\n",
    "        src = os.path.join(src_path, fname)\n",
    "        dst = os.path.join(dst_path, fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9fc1117-7c6d-4775-9499-b442e1367c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label in range(CLASS_COUNT):\n",
    "    class_name = CLASS_LIST[class_label]\n",
    "\n",
    "    src_path = os.path.join(IMAGE_PATH, class_name)\n",
    "    total_count = len(os.listdir(src_path))\n",
    "\n",
    "    train_end = int(total_count * 0.7)\n",
    "    val_end = int(total_count * 0.85)\n",
    "\n",
    "    create_dataset(IMAGE_PATH, train_dir, class_name, 0, train_end)\n",
    "    create_dataset(IMAGE_PATH, validation_dir, class_name, train_end, val_end)\n",
    "    create_dataset(IMAGE_PATH, test_dir, class_name, val_end, total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97774a2c-3783-4fb1-a756-662954466a57",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e7784-ec68-451f-a4f4-c448700922ab",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa59dbeb-faa4-4d2d-a316-969b880bec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 160\n",
    "IMG_HEIGHT = 160\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a35a3238-6dca-4a51-a512-84280e920586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = MobileNet(include_top=False, input_shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
    "\n",
    "    return models.Model(inputs=input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2353b50b-b845-4105-aba7-aa2611ae12fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_160_tf_no_top.h5\n",
      "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = model_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fee5ccbd-fda6-4a72-b878-b2c37bc73662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenet_1.00_160 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenet_1.00_160 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m3,228,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,294,594</span> (12.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,294,594\u001b[0m (12.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,730</span> (256.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,730\u001b[0m (256.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> (12.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,228,864\u001b[0m (12.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d9f36-a81b-49a6-bc7a-9ff344e8f315",
   "metadata": {},
   "source": [
    "## Image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "285a9252-0a5d-45a1-9971-46c67a0657eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5603 images belonging to 2 classes.\n",
      "Found 1201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(160, 160),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(160, 160),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8babe17-0b1f-4bfb-ab60-a8751b46ac88",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3917e6-8989-462a-9de7-bcc7f4117e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем Adam оптимизатор (как рекомендовано в задании)\n",
    "# Для fine-tuning предобученных моделей обычно используют learning_rate от 1e-4 до 2e-4\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d443009-824e-4e69-b806-02816e977f7e",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb6067-b5a0-4f5e-8b31-f6b46d77be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 97ms/step - acc: 0.7077 - loss: 0.5814 - val_acc: 0.9151 - val_loss: 0.2648\n",
      "Epoch 2/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - acc: 0.8376 - loss: 0.3719 - val_acc: 0.9442 - val_loss: 0.1753\n",
      "Epoch 3/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - acc: 0.8810 - loss: 0.2892 - val_acc: 0.9609 - val_loss: 0.1355\n",
      "Epoch 4/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 106ms/step - acc: 0.8986 - loss: 0.2490 - val_acc: 0.9642 - val_loss: 0.1117\n",
      "Epoch 5/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 109ms/step - acc: 0.9109 - loss: 0.2232 - val_acc: 0.9700 - val_loss: 0.0974\n",
      "Epoch 6/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 112ms/step - acc: 0.9147 - loss: 0.2030 - val_acc: 0.9709 - val_loss: 0.0875\n",
      "Epoch 7/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 114ms/step - acc: 0.9252 - loss: 0.1807 - val_acc: 0.9709 - val_loss: 0.0831\n",
      "Epoch 8/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 114ms/step - acc: 0.9272 - loss: 0.1723 - val_acc: 0.9767 - val_loss: 0.0747\n",
      "Epoch 9/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 113ms/step - acc: 0.9372 - loss: 0.1646 - val_acc: 0.9759 - val_loss: 0.0712\n",
      "Epoch 10/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 114ms/step - acc: 0.9331 - loss: 0.1633 - val_acc: 0.9784 - val_loss: 0.0673\n",
      "Epoch 11/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 115ms/step - acc: 0.9352 - loss: 0.1557 - val_acc: 0.9792 - val_loss: 0.0643\n",
      "Epoch 12/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9388 - loss: 0.1566 - val_acc: 0.9784 - val_loss: 0.0626\n",
      "Epoch 13/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9422 - loss: 0.1450 - val_acc: 0.9784 - val_loss: 0.0603\n",
      "Epoch 14/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9402 - loss: 0.1527 - val_acc: 0.9800 - val_loss: 0.0594\n",
      "Epoch 15/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9397 - loss: 0.1435 - val_acc: 0.9792 - val_loss: 0.0579\n",
      "Epoch 16/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9463 - loss: 0.1433 - val_acc: 0.9808 - val_loss: 0.0570\n",
      "Epoch 17/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9456 - loss: 0.1372 - val_acc: 0.9808 - val_loss: 0.0559\n",
      "Epoch 18/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9497 - loss: 0.1312 - val_acc: 0.9808 - val_loss: 0.0549\n",
      "Epoch 19/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9540 - loss: 0.1237 - val_acc: 0.9825 - val_loss: 0.0545\n",
      "Epoch 20/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9498 - loss: 0.1235 - val_acc: 0.9792 - val_loss: 0.0525\n",
      "Epoch 21/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9477 - loss: 0.1344 - val_acc: 0.9825 - val_loss: 0.0529\n",
      "Epoch 22/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9481 - loss: 0.1333 - val_acc: 0.9808 - val_loss: 0.0511\n",
      "Epoch 23/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9459 - loss: 0.1298 - val_acc: 0.9825 - val_loss: 0.0517\n",
      "Epoch 24/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 118ms/step - acc: 0.9468 - loss: 0.1308 - val_acc: 0.9825 - val_loss: 0.0516\n",
      "Epoch 25/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9500 - loss: 0.1237 - val_acc: 0.9808 - val_loss: 0.0504\n",
      "Epoch 26/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9540 - loss: 0.1182 - val_acc: 0.9817 - val_loss: 0.0493\n",
      "Epoch 27/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 116ms/step - acc: 0.9502 - loss: 0.1186 - val_acc: 0.9825 - val_loss: 0.0487\n",
      "Epoch 28/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 118ms/step - acc: 0.9506 - loss: 0.1209 - val_acc: 0.9833 - val_loss: 0.0490\n",
      "Epoch 29/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 117ms/step - acc: 0.9565 - loss: 0.1140 - val_acc: 0.9833 - val_loss: 0.0483\n",
      "Epoch 30/30\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 119ms/step - acc: 0.9548 - loss: 0.1187 - val_acc: 0.9825 - val_loss: 0.0481\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping для автоматической остановки при отсутствии улучшений\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_acc',  # отслеживаем валидационную точность\n",
    "    patience=5,  # ждем 5 эпох без улучшения\n",
    "    restore_best_weights=True,  # восстанавливаем веса лучшей модели\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # уменьшено с 30 до 20 (модель достигает 95%+ уже на 3-й эпохе)\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]  # добавляем callback для ранней остановки\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ab014de-e158-4e39-889c-41d52f437ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(160, 160),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078c06c-692f-4dde-994b-88d15e505a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - acc: 0.9650 - loss: 0.0768\n"
     ]
    }
   ],
   "source": [
    "# Оцениваем на всей тестовой выборке (1201 изображение / 20 batch_size = ~61 шагов)\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'\\nФинальная точность на тестовой выборке: {test_acc:.4f} ({test_acc*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv_nn)",
   "language": "python",
   "name": "venv_nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
